# Alert Configuration
# Defines alert rules, thresholds, and notification channels

# Notification Channels
channels:
  slack:
    enabled: true
    webhook_url: ${SLACK_WEBHOOK_URL}  # Set in environment variables
    default_channel: "#trading-alerts"
    mention_on_critical: "@channel"
  
  email:
    enabled: false
    smtp_server: "smtp.gmail.com"
    smtp_port: 587
    sender_email: ${ALERT_EMAIL_SENDER}
    recipient_emails:
      - ${ALERT_EMAIL_RECIPIENT}
    use_tls: true
  
  # Future: PagerDuty, Opsgenie, etc.
  pagerduty:
    enabled: false
    api_key: ${PAGERDUTY_API_KEY}

# Alert Severity Levels
severity_levels:
  info:
    color: "#36a64f"  # Green
    channels: ["slack"]
  warning:
    color: "#ff9900"  # Orange
    channels: ["slack", "email"]
  critical:
    color: "#ff0000"  # Red
    channels: ["slack", "email"]
    mention: "@channel"

# Trading Alerts

# 1. Kill-Switch Alerts
kill_switch:
  enabled: true
  severity: "critical"
  
  daily_drawdown:
    threshold_pct: 2.0
    message: "ğŸš¨ KILL-SWITCH ACTIVATED: Daily drawdown exceeded {drawdown_pct:.2f}% (limit: {threshold_pct:.2f}%)"
  
  broker_api_errors:
    consecutive_failures: 5
    message: "ğŸš¨ KILL-SWITCH ACTIVATED: Broker API errors ({count} consecutive failures)"
  
  data_feed_disconnect:
    disconnect_seconds: 120
    message: "ğŸš¨ KILL-SWITCH ACTIVATED: Data feed disconnected for {disconnect_seconds}s"

# 2. Risk Alerts
risk:
  enabled: true
  
  approaching_drawdown:
    severity: "warning"
    threshold_pct: 1.5  # Alert at 1.5% DD (before 2% kill-switch)
    message: "âš ï¸ Daily drawdown approaching limit: {drawdown_pct:.2f}% (kill-switch at 2.0%)"
  
  high_exposure:
    severity: "warning"
    threshold_pct: 80  # Alert if exposure > 80% of limit
    message: "âš ï¸ High exposure: {exposure_pct:.1f}% (limit: {limit_pct:.1f}%)"
  
  consecutive_losses:
    severity: "warning"
    count: 3
    message: "âš ï¸ {count} consecutive losing trades. Consider pausing strategy."
  
  position_concentration:
    severity: "warning"
    max_position_pct: 10  # Alert if single position > 10% of portfolio
    message: "âš ï¸ Position concentration: {symbol} is {position_pct:.1f}% of portfolio"

# 3. Execution Alerts
execution:
  enabled: true
  
  high_rejection_rate:
    severity: "warning"
    threshold_pct: 5.0  # > 5% of orders rejected in last hour
    lookback_minutes: 60
    message: "âš ï¸ High order rejection rate: {rejection_pct:.1f}% in last {lookback_minutes} minutes"
  
  high_slippage:
    severity: "warning"
    threshold_bps: 15  # Average slippage > 15 bps
    lookback_minutes: 60
    message: "âš ï¸ High slippage: {slippage_bps:.1f} bps average in last {lookback_minutes} minutes"
  
  low_fill_rate:
    severity: "warning"
    threshold_pct: 70  # < 70% fill rate
    lookback_minutes: 60
    message: "âš ï¸ Low fill rate: {fill_rate_pct:.1f}% in last {lookback_minutes} minutes"
  
  order_timeout:
    severity: "warning"
    count: 10  # > 10 orders timed out in last hour
    lookback_minutes: 60
    message: "âš ï¸ {count} orders timed out in last {lookback_minutes} minutes"
  
  partial_fill_rate:
    severity: "info"
    threshold_pct: 20  # > 20% of orders partially filled
    lookback_minutes: 60
    message: "â„¹ï¸ High partial fill rate: {partial_fill_pct:.1f}% in last {lookback_minutes} minutes"

# 4. System Health Alerts
system:
  enabled: true
  
  data_feed_latency:
    severity: "warning"
    p95_threshold_ms: 500  # P95 latency > 500ms
    lookback_minutes: 15
    message: "âš ï¸ Data feed latency high: P95 = {p95_latency_ms:.0f}ms in last {lookback_minutes} minutes"
  
  order_latency:
    severity: "warning"
    median_threshold_ms: 200  # Median latency > 200ms
    lookback_minutes: 15
    message: "âš ï¸ Order placement latency high: Median = {median_latency_ms:.0f}ms in last {lookback_minutes} minutes"
  
  memory_usage:
    severity: "warning"
    threshold_pct: 80  # > 80% of 64GB
    message: "âš ï¸ High memory usage: {memory_pct:.1f}%"
  
  disk_space:
    severity: "critical"
    threshold_pct: 10  # < 10% free space
    message: "ğŸš¨ Low disk space: {free_pct:.1f}% remaining"
  
  cpu_usage:
    severity: "warning"
    threshold_pct: 90  # > 90% sustained CPU usage
    duration_seconds: 300  # For 5 minutes
    message: "âš ï¸ High CPU usage: {cpu_pct:.1f}% for {duration_seconds}s"
  
  component_failure:
    severity: "critical"
    components: ["data_feed", "execution_engine", "risk_manager", "portfolio"]
    message: "ğŸš¨ Component failure: {component} is {status}"

# 5. Model and ML Alerts
ml:
  enabled: true
  
  model_promotion:
    severity: "info"
    message: "âœ… Model promoted: {model_id} for {strategy} (OOS Sharpe: {sharpe:.2f})"
  
  promotion_gate_failure:
    severity: "warning"
    message: "âš ï¸ Model promotion failed: {model_id} for {strategy}. Failed gates: {failed_gates}"
  
  model_drift:
    severity: "warning"
    ks_threshold: 0.2
    psi_threshold: 0.3
    message: "âš ï¸ Model drift detected: {model_id}. KS: {ks:.3f}, PSI: {psi:.3f}"
  
  model_demotion:
    severity: "critical"
    message: "ğŸš¨ Model demoted: {model_id} for {strategy}. Reason: {reason}"
  
  retraining_triggered:
    severity: "info"
    message: "â„¹ï¸ Model retraining triggered: {strategy}. Trigger: {trigger_reason}"
  
  retraining_failed:
    severity: "critical"
    message: "ğŸš¨ Model retraining failed: {strategy}. Error: {error}"

# 6. Performance Alerts
performance:
  enabled: true
  
  daily_pnl_threshold:
    severity: "info"
    threshold_pct: 5.0  # Alert if daily P&L exceeds +/- 5%
    message: "â„¹ï¸ Daily P&L: {pnl_pct:+.2f}% (${pnl_usd:,.2f})"
  
  weekly_review:
    severity: "info"
    schedule: "sunday_evening"  # Send weekly summary
    message: "ğŸ“Š Weekly Summary: P&L {pnl_pct:+.2f}%, Sharpe {sharpe:.2f}, Max DD {max_dd:.2f}%"
  
  strategy_performance_degradation:
    severity: "warning"
    sharpe_threshold: 0.5  # < 0.5 Sharpe over 20 days
    lookback_days: 20
    message: "âš ï¸ Strategy performance degradation: {strategy} Sharpe = {sharpe:.2f} over {lookback_days} days"

# 7. Universe and Data Alerts
universe:
  enabled: true
  
  universe_contraction:
    severity: "warning"
    min_core_symbols: 150  # Alert if Core universe < 150 (target: 200)
    message: "âš ï¸ Universe contraction: Core tier has {core_count} symbols (target: 200)"
  
  high_earnings_blackout:
    severity: "info"
    threshold_pct: 10  # > 10% of universe in earnings blackout
    message: "â„¹ï¸ High earnings blackout: {blackout_pct:.1f}% of universe"
  
  shortability_issues:
    severity: "warning"
    threshold_pct: 70  # < 70% of universe is shortable
    message: "âš ï¸ Shortability issues: Only {shortable_pct:.1f}% of universe is shortable"
  
  data_quality_issue:
    severity: "critical"
    message: "ğŸš¨ Data quality issue: {issue_description} for {affected_symbols}"

# 8. Operational Alerts
operations:
  enabled: true
  
  manual_intervention:
    severity: "warning"
    message: "âš ï¸ Manual intervention: {action} by {user}. Reason: {reason}"
  
  trading_paused:
    severity: "warning"
    message: "âš ï¸ Trading paused. Reason: {reason}"
  
  trading_resumed:
    severity: "info"
    message: "âœ… Trading resumed by {user}"
  
  emergency_halt:
    severity: "critical"
    message: "ğŸš¨ EMERGENCY HALT activated. Reason: {reason}. Positions flattened: {flattened}"
  
  backup_failed:
    severity: "critical"
    message: "ğŸš¨ Backup failed: {backup_type}. Error: {error}"
  
  scheduler_task_failed:
    severity: "warning"
    message: "âš ï¸ Scheduled task failed: {task_name}. Error: {error}"

# 9. Connection Alerts
connectivity:
  enabled: true
  
  data_feed_reconnection:
    severity: "info"
    message: "â„¹ï¸ Data feed reconnected after {disconnect_seconds}s"
  
  broker_api_failover:
    severity: "warning"
    message: "âš ï¸ Broker API failover: Switched to backup source"
  
  data_source_failover:
    severity: "warning"
    message: "âš ï¸ Data source failover: Switched from {primary} to {backup}"

# Alert Throttling (prevent spam)
throttling:
  enabled: true
  
  # Maximum alerts per channel per time window
  max_alerts_per_minute: 10
  max_alerts_per_hour: 50
  
  # Deduplicate identical alerts within time window
  dedupe_window_seconds: 300  # 5 minutes
  
  # Group similar alerts (e.g., multiple high slippage alerts)
  group_similar_alerts: true
  group_window_seconds: 600  # 10 minutes

# Alert Routing (route specific alerts to specific channels)
routing:
  # Critical alerts always go to all channels
  critical_to_all: true
  
  # Custom routing rules
  rules:
    - name: "model_alerts_to_ml_channel"
      alert_types: ["model_promotion", "model_drift", "model_demotion"]
      channels: ["slack"]
      slack_channel: "#ml-models"
    
    - name: "performance_to_reports_channel"
      alert_types: ["daily_pnl_threshold", "weekly_review"]
      channels: ["slack"]
      slack_channel: "#performance-reports"

# Alert Testing
testing:
  # Send test alert on startup
  send_test_on_startup: false
  test_message: "âœ… Trading system started. Alert system is operational."
  
  # Test alert command (for manual testing)
  test_command_enabled: true

# Alert History
history:
  # Store alert history for review
  enabled: true
  retention_days: 90
  storage_path: "./logs/alert_history.db"

# Notes
notes: |
  Alert Configuration Best Practices:
  1. Set conservative thresholds initially (avoid alert fatigue)
  2. Review alert frequency weekly; adjust thresholds if too noisy
  3. Ensure critical alerts are routed to multiple channels
  4. Test alerts regularly (use test_command)
  5. Document alert response procedures in ops playbook
  
  Alert Response Times (target):
  - Critical: < 5 minutes
  - Warning: < 30 minutes
  - Info: < 24 hours (next business day)
