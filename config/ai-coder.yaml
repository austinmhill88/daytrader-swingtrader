backend: "ollama"
ollama_base_url: "http://127.0.0.1:11434"

default_model: "phi3:mini-4k-q4"
models:
  - alias: "phi3:mini-4k-q4"
    ollama_model: "phi3-local"   # the local GPU model you created

runtime_defaults:
  temperature: 0.3
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  num_ctx: 4096
  num_predict: 256   # increase to 320 or 384 if you want even longer

allow_network: true
allow_html: false
root_dir: "."
allow_write: false
allowed_globs: []
denied_globs: []